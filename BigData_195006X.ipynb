{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPCk5pizvvJsT7uBJsqYlbQ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sachinibuddhika/Big-Data-Project-Analysis-on-Swiggy-Zomato-Order-Information-Dataseton/blob/main/BigData_195006X.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Mount google drive**"
      ],
      "metadata": {
        "id": "44PHZ0IiXthT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wtNiD9KcXjGM",
        "outputId": "1a73120a-4b3a-4a66-9791-fc7a36fa1a6f"
      },
      "execution_count": 324,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Initialize spark context**"
      ],
      "metadata": {
        "id": "rimobtKzZfEr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#install pyspark\n",
        "!pip install pyspark\n",
        "\n",
        "# Import necessary libraries\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "reEDWFGeZ7_6",
        "outputId": "f58834b3-a31b-49f5-eec0-fae6be3eed68"
      },
      "execution_count": 325,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.10/dist-packages (3.5.1)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a SparkSession\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"OrderAnalysis\") \\\n",
        "    .getOrCreate()"
      ],
      "metadata": {
        "id": "cQaU4falevl8"
      },
      "execution_count": 326,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 327,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 219
        },
        "id": "MAlgKlOER4OO",
        "outputId": "bd10b68e-a616-4222-922d-60c2a046d6f1"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pyspark.sql.session.SparkSession at 0x7e828c7ae770>"
            ],
            "text/html": [
              "\n",
              "            <div>\n",
              "                <p><b>SparkSession - in-memory</b></p>\n",
              "                \n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://eab59ba0d86d:4040\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v3.5.1</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local[*]</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>OrderAnalysis</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        \n",
              "            </div>\n",
              "        "
            ]
          },
          "metadata": {},
          "execution_count": 327
        }
      ],
      "source": [
        "spark"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "conf = spark.sparkContext.getConf()\n",
        "\n",
        "# Print the configuration settings\n",
        "print(\"spark.app.name = \", conf.get(\"spark.app.name\"))\n",
        "print(\"spark.master = \", conf.get(\"spark.master\"))\n",
        "print(\"spark.executor.memory = \", conf.get(\"spark.executor.memory\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ev9QeQSkkHCG",
        "outputId": "650ab58d-4cb5-4a44-96a2-ae6e9753319b"
      },
      "execution_count": 328,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "spark.app.name =  OrderAnalysis\n",
            "spark.master =  local[*]\n",
            "spark.executor.memory =  None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the CSV file into a DataFrame\n",
        "df = spark.read.csv('/content/drive/My Drive/BigData/DataSet/Rider-Info.csv', header=True)\n"
      ],
      "metadata": {
        "id": "_PZZpMJNkade"
      },
      "execution_count": 329,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Read the dataset**"
      ],
      "metadata": {
        "id": "SREMofDAkkjO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#specifying the dataset to read\n",
        "help(spark.read.csv)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VCtvd8gVlJhW",
        "outputId": "4974c4ac-ccc5-48ad-d3a6-00d0c4f6dc1a"
      },
      "execution_count": 330,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Help on method csv in module pyspark.sql.readwriter:\n",
            "\n",
            "csv(path: Union[str, List[str]], schema: Union[pyspark.sql.types.StructType, str, NoneType] = None, sep: Optional[str] = None, encoding: Optional[str] = None, quote: Optional[str] = None, escape: Optional[str] = None, comment: Optional[str] = None, header: Union[bool, str, NoneType] = None, inferSchema: Union[bool, str, NoneType] = None, ignoreLeadingWhiteSpace: Union[bool, str, NoneType] = None, ignoreTrailingWhiteSpace: Union[bool, str, NoneType] = None, nullValue: Optional[str] = None, nanValue: Optional[str] = None, positiveInf: Optional[str] = None, negativeInf: Optional[str] = None, dateFormat: Optional[str] = None, timestampFormat: Optional[str] = None, maxColumns: Union[str, int, NoneType] = None, maxCharsPerColumn: Union[str, int, NoneType] = None, maxMalformedLogPerPartition: Union[str, int, NoneType] = None, mode: Optional[str] = None, columnNameOfCorruptRecord: Optional[str] = None, multiLine: Union[bool, str, NoneType] = None, charToEscapeQuoteEscaping: Optional[str] = None, samplingRatio: Union[str, float, NoneType] = None, enforceSchema: Union[bool, str, NoneType] = None, emptyValue: Optional[str] = None, locale: Optional[str] = None, lineSep: Optional[str] = None, pathGlobFilter: Union[bool, str, NoneType] = None, recursiveFileLookup: Union[bool, str, NoneType] = None, modifiedBefore: Union[bool, str, NoneType] = None, modifiedAfter: Union[bool, str, NoneType] = None, unescapedQuoteHandling: Optional[str] = None) -> 'DataFrame' method of pyspark.sql.readwriter.DataFrameReader instance\n",
            "    Loads a CSV file and returns the result as a  :class:`DataFrame`.\n",
            "    \n",
            "    This function will go through the input once to determine the input schema if\n",
            "    ``inferSchema`` is enabled. To avoid going through the entire data once, disable\n",
            "    ``inferSchema`` option or specify the schema explicitly using ``schema``.\n",
            "    \n",
            "    .. versionadded:: 2.0.0\n",
            "    \n",
            "    .. versionchanged:: 3.4.0\n",
            "        Supports Spark Connect.\n",
            "    \n",
            "    Parameters\n",
            "    ----------\n",
            "    path : str or list\n",
            "        string, or list of strings, for input path(s),\n",
            "        or RDD of Strings storing CSV rows.\n",
            "    schema : :class:`pyspark.sql.types.StructType` or str, optional\n",
            "        an optional :class:`pyspark.sql.types.StructType` for the input schema\n",
            "        or a DDL-formatted string (For example ``col0 INT, col1 DOUBLE``).\n",
            "    \n",
            "    Other Parameters\n",
            "    ----------------\n",
            "    Extra options\n",
            "        For the extra options, refer to\n",
            "        `Data Source Option <https://spark.apache.org/docs/latest/sql-data-sources-csv.html#data-source-option>`_\n",
            "        for the version you use.\n",
            "    \n",
            "        .. # noqa\n",
            "    \n",
            "    Examples\n",
            "    --------\n",
            "    Write a DataFrame into a CSV file and read it back.\n",
            "    \n",
            "    >>> import tempfile\n",
            "    >>> with tempfile.TemporaryDirectory() as d:\n",
            "    ...     # Write a DataFrame into a CSV file\n",
            "    ...     df = spark.createDataFrame([{\"age\": 100, \"name\": \"Hyukjin Kwon\"}])\n",
            "    ...     df.write.mode(\"overwrite\").format(\"csv\").save(d)\n",
            "    ...\n",
            "    ...     # Read the CSV file as a DataFrame with 'nullValue' option set to 'Hyukjin Kwon'.\n",
            "    ...     spark.read.csv(d, schema=df.schema, nullValue=\"Hyukjin Kwon\").show()\n",
            "    +---+----+\n",
            "    |age|name|\n",
            "    +---+----+\n",
            "    |100|NULL|\n",
            "    +---+----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the CSV file into a DataFrame\n",
        "df = spark.read.csv('/content/drive/My Drive/BigData/DataSet/Rider-Info.csv', header=True)"
      ],
      "metadata": {
        "id": "5-tAnbSHkitO"
      },
      "execution_count": 331,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.show(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kOtxBbySlc2V",
        "outputId": "eedb9c0e-d8ef-481c-bed7-54be4df705de"
      },
      "execution_count": 332,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------------+--------+-------------------+-------------------+-------------------+-------------------+-------------------+--------+-------------------+------------------+--------------+----------------+---------+------------------+--------------------+-------------------+-------------------+----------------+------------------+--------------+\n",
            "|         order_time|order_id|         order_date|         allot_time|        accept_time|        pickup_time|     delivered_time|rider_id|first_mile_distance|last_mile_distance|alloted_orders|delivered_orders|cancelled|undelivered_orders|lifetime_order_count|reassignment_method|reassignment_reason|reassigned_order|      session_time|cancelled_time|\n",
            "+-------------------+--------+-------------------+-------------------+-------------------+-------------------+-------------------+--------+-------------------+------------------+--------------+----------------+---------+------------------+--------------------+-------------------+-------------------+----------------+------------------+--------------+\n",
            "|2021-01-26 02:21:35|  556753|2021-01-26 00:00:00|2021-01-26 02:21:59|2021-01-26 02:22:08|2021-01-26 02:32:51|2021-01-26 02:49:47|   11696|             1.5666|              2.65|          46.0|            46.0|        0|               0.0|               621.0|               NULL|               NULL|            NULL|              NULL|          NULL|\n",
            "|2021-01-26 02:33:16|  556754|2021-01-26 00:00:00|2021-01-26 02:33:57|2021-01-26 02:34:45|2021-01-26 02:50:25|2021-01-26 03:11:15|   18117|             2.5207|              2.76|           8.0|             8.0|        0|               0.0|               105.0|               NULL|               NULL|            NULL|3.2666666666666666|          NULL|\n",
            "|2021-01-26 02:39:49|  556755|2021-01-26 00:00:00|2021-01-26 02:39:57|2021-01-26 02:40:13|2021-01-26 02:56:00|2021-01-26 03:12:46|   18623|             2.2074|               4.8|           1.0|             1.0|        0|               0.0|                66.0|               NULL|               NULL|            NULL| 9.816666666666666|          NULL|\n",
            "|2021-01-26 02:47:53|  556756|2021-01-26 00:00:00|2021-01-26 02:48:25|2021-01-26 02:49:06|2021-01-26 03:21:51|2021-01-26 03:41:05|   15945|             2.1894|              6.38|           1.0|             1.0|        0|               0.0|               127.0|               NULL|               NULL|            NULL|17.533333333333335|          NULL|\n",
            "|2021-01-26 03:06:30|  556757|2021-01-26 00:00:00|2021-01-26 03:07:21|2021-01-26 03:07:57|2021-01-26 03:31:38|2021-01-26 04:00:15|   17589|              2.787|              4.01|          34.0|            34.0|        0|               0.0|                84.0|               NULL|               NULL|            NULL|              1.35|          NULL|\n",
            "+-------------------+--------+-------------------+-------------------+-------------------+-------------------+-------------------+--------+-------------------+------------------+--------------+----------------+---------+------------------+--------------------+-------------------+-------------------+----------------+------------------+--------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.columns\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gZmxqq71Wzh6",
        "outputId": "9c3795fe-116b-4efb-e64b-5fb8705bbe7b"
      },
      "execution_count": 333,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['order_time',\n",
              " 'order_id',\n",
              " 'order_date',\n",
              " 'allot_time',\n",
              " 'accept_time',\n",
              " 'pickup_time',\n",
              " 'delivered_time',\n",
              " 'rider_id',\n",
              " 'first_mile_distance',\n",
              " 'last_mile_distance',\n",
              " 'alloted_orders',\n",
              " 'delivered_orders',\n",
              " 'cancelled',\n",
              " 'undelivered_orders',\n",
              " 'lifetime_order_count',\n",
              " 'reassignment_method',\n",
              " 'reassignment_reason',\n",
              " 'reassigned_order',\n",
              " 'session_time',\n",
              " 'cancelled_time']"
            ]
          },
          "metadata": {},
          "execution_count": 333
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Preprocess data**"
      ],
      "metadata": {
        "id": "0QFEouj2KeYB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import functions as F"
      ],
      "metadata": {
        "id": "VoK69JOAktns"
      },
      "execution_count": 334,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dropping columns"
      ],
      "metadata": {
        "id": "bPBPl2JtK6Ad"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "columns_to_drop = ['accept_time', 'undelivered_orders','reassignment_method', 'reassignment_reason','cancelled_time']\n",
        "df2 = df.drop(*columns_to_drop)\n",
        "\n",
        "df2.show(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "addykTNzfCHK",
        "outputId": "a75931e9-ce86-4ab1-b794-033c751ae97b"
      },
      "execution_count": 335,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------------+--------+-------------------+-------------------+-------------------+-------------------+--------+-------------------+------------------+--------------+----------------+---------+--------------------+----------------+------------------+\n",
            "|         order_time|order_id|         order_date|         allot_time|        pickup_time|     delivered_time|rider_id|first_mile_distance|last_mile_distance|alloted_orders|delivered_orders|cancelled|lifetime_order_count|reassigned_order|      session_time|\n",
            "+-------------------+--------+-------------------+-------------------+-------------------+-------------------+--------+-------------------+------------------+--------------+----------------+---------+--------------------+----------------+------------------+\n",
            "|2021-01-26 02:21:35|  556753|2021-01-26 00:00:00|2021-01-26 02:21:59|2021-01-26 02:32:51|2021-01-26 02:49:47|   11696|             1.5666|              2.65|          46.0|            46.0|        0|               621.0|            NULL|              NULL|\n",
            "|2021-01-26 02:33:16|  556754|2021-01-26 00:00:00|2021-01-26 02:33:57|2021-01-26 02:50:25|2021-01-26 03:11:15|   18117|             2.5207|              2.76|           8.0|             8.0|        0|               105.0|            NULL|3.2666666666666666|\n",
            "|2021-01-26 02:39:49|  556755|2021-01-26 00:00:00|2021-01-26 02:39:57|2021-01-26 02:56:00|2021-01-26 03:12:46|   18623|             2.2074|               4.8|           1.0|             1.0|        0|                66.0|            NULL| 9.816666666666666|\n",
            "|2021-01-26 02:47:53|  556756|2021-01-26 00:00:00|2021-01-26 02:48:25|2021-01-26 03:21:51|2021-01-26 03:41:05|   15945|             2.1894|              6.38|           1.0|             1.0|        0|               127.0|            NULL|17.533333333333335|\n",
            "|2021-01-26 03:06:30|  556757|2021-01-26 00:00:00|2021-01-26 03:07:21|2021-01-26 03:31:38|2021-01-26 04:00:15|   17589|              2.787|              4.01|          34.0|            34.0|        0|                84.0|            NULL|              1.35|\n",
            "+-------------------+--------+-------------------+-------------------+-------------------+-------------------+--------+-------------------+------------------+--------------+----------------+---------+--------------------+----------------+------------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df2.columns"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dJLcxbhKfWl4",
        "outputId": "9d6f3247-4539-4c73-9bbd-90e0e2259006"
      },
      "execution_count": 336,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['order_time',\n",
              " 'order_id',\n",
              " 'order_date',\n",
              " 'allot_time',\n",
              " 'pickup_time',\n",
              " 'delivered_time',\n",
              " 'rider_id',\n",
              " 'first_mile_distance',\n",
              " 'last_mile_distance',\n",
              " 'alloted_orders',\n",
              " 'delivered_orders',\n",
              " 'cancelled',\n",
              " 'lifetime_order_count',\n",
              " 'reassigned_order',\n",
              " 'session_time']"
            ]
          },
          "metadata": {},
          "execution_count": 336
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dropping null rows"
      ],
      "metadata": {
        "id": "0p42b_u0ft4r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# display null values\n",
        "df2.select([F.count(F.when(F.col(c).isNull(), c)).alias(c) for c in df2.columns]).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SZk6Ns7bzVJq",
        "outputId": "f136032e-7035-4f5c-87da-8d979beb1488"
      },
      "execution_count": 337,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+--------+----------+----------+-----------+--------------+--------+-------------------+------------------+--------------+----------------+---------+--------------------+----------------+------------+\n",
            "|order_time|order_id|order_date|allot_time|pickup_time|delivered_time|rider_id|first_mile_distance|last_mile_distance|alloted_orders|delivered_orders|cancelled|lifetime_order_count|reassigned_order|session_time|\n",
            "+----------+--------+----------+----------+-----------+--------------+--------+-------------------+------------------+--------------+----------------+---------+--------------------+----------------+------------+\n",
            "|         0|       0|         0|         0|       2421|          5218|       0|                  0|                 0|         16948|           17341|        0|                  53|          436247|        3675|\n",
            "+----------+--------+----------+----------+-----------+--------------+--------+-------------------+------------------+--------------+----------------+---------+--------------------+----------------+------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df2 = df2.filter(df.accept_time.isNotNull())"
      ],
      "metadata": {
        "id": "RSTWh4G7fxAb"
      },
      "execution_count": 338,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rows_to_drop = ['pickup_time','delivered_time']"
      ],
      "metadata": {
        "id": "k2u_doCZfzHK"
      },
      "execution_count": 339,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dT4SSg3NjMD0"
      },
      "execution_count": 339,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for row in rows_to_drop:\n",
        "  df2 = df2.filter(df[row].isNotNull())"
      ],
      "metadata": {
        "id": "W6gadqS_f9qi"
      },
      "execution_count": 340,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# display null values\n",
        "df2.select([F.count(F.when(F.col(c).isNull(), c)).alias(c) for c in df2.columns]).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "habeqb9Kf_oL",
        "outputId": "88360749-ea41-4c77-9c5c-429cefc28251"
      },
      "execution_count": 341,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+--------+----------+----------+-----------+--------------+--------+-------------------+------------------+--------------+----------------+---------+--------------------+----------------+------------+\n",
            "|order_time|order_id|order_date|allot_time|pickup_time|delivered_time|rider_id|first_mile_distance|last_mile_distance|alloted_orders|delivered_orders|cancelled|lifetime_order_count|reassigned_order|session_time|\n",
            "+----------+--------+----------+----------+-----------+--------------+--------+-------------------+------------------+--------------+----------------+---------+--------------------+----------------+------------+\n",
            "|         0|       0|         0|         0|          0|             0|       0|                  0|                 0|         16299|           16486|        0|                   2|          431751|        3531|\n",
            "+----------+--------+----------+----------+-----------+--------------+--------+-------------------+------------------+--------------+----------------+---------+--------------------+----------------+------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df2.count()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "paiwnX4yjeQ0",
        "outputId": "7e8c1df8-db32-4f54-82f3-6ff810fda386"
      },
      "execution_count": 342,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "444782"
            ]
          },
          "metadata": {},
          "execution_count": 342
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Filling missing values"
      ],
      "metadata": {
        "id": "vduEtPL_jxbk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import mean"
      ],
      "metadata": {
        "id": "jDj6WxenlVvD"
      },
      "execution_count": 343,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fill missing values with mean in specified columns\n",
        "columns_to_fill = ['alloted_orders', 'delivered_orders', 'lifetime_order_count', 'session_time']\n",
        "mean_values = df2.select([mean(col).alias(col) for col in columns_to_fill]).collect()[0].asDict()\n",
        "df2 = df2.fillna(mean_values)"
      ],
      "metadata": {
        "id": "uCW-6zavlAWM"
      },
      "execution_count": 344,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# display null values\n",
        "df2.select([F.count(F.when(F.col(c).isNull(), c)).alias(c) for c in df2.columns]).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "073ypiK7lbhN",
        "outputId": "da9c53b1-4449-4a3f-c023-8ef5e0975648"
      },
      "execution_count": 345,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+--------+----------+----------+-----------+--------------+--------+-------------------+------------------+--------------+----------------+---------+--------------------+----------------+------------+\n",
            "|order_time|order_id|order_date|allot_time|pickup_time|delivered_time|rider_id|first_mile_distance|last_mile_distance|alloted_orders|delivered_orders|cancelled|lifetime_order_count|reassigned_order|session_time|\n",
            "+----------+--------+----------+----------+-----------+--------------+--------+-------------------+------------------+--------------+----------------+---------+--------------------+----------------+------------+\n",
            "|         0|       0|         0|         0|          0|             0|       0|                  0|                 0|             0|               0|        0|                   0|          431751|           0|\n",
            "+----------+--------+----------+----------+-----------+--------------+--------+-------------------+------------------+--------------+----------------+---------+--------------------+----------------+------------+\n",
            "\n"
          ]
        }
      ]
    }
  ]
}